# -*- coding: utf-8 -*-
"""Schemes_Embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ECZpIQLC_8ju0ah45GbGGOjPH8uuH2-u
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install sentence-transformers

import json
import re
import numpy as np
import string
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sentence_transformers import SentenceTransformer

# NLTK resources are download
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

# Load scraped data from JSON file
with open('/content/drive/MyDrive/Final_Year_Project/myschemes_scraped.json', 'r', encoding='utf-8') as file:
    scraped_data = json.load(file)

# Preprocessing function
def preprocess_text(text):
    # Remove escape characters
    text = re.sub(r'\n', ' ', text)
    text = re.sub(r'\u2019', "'", text)

    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))

    # Tokenization and lowercasing
    tokens = word_tokenize(text.lower())

    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]

    # Lemmatization
    lemmatizer = WordNetLemmatizer()
    tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return ' '.join(tokens)

# Initialize Sentence Transformer model
model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')

# Preprocess and generate embeddings
embeddings = []
metadata = []

for data in scraped_data:
    try:
        text = ' '.join([
            f"<scheme_name> {data.get('scheme_name', '')}",
            f"<details> {data.get('details', '')}",
            f"<benefits> {data.get('benefits', '')}",
            f"<eligibility> {data.get('eligibility', '')}",
            f"<application_process> {data.get('application_process', '')}",
            f"<documents_required> {data.get('documents_required', '')}"
        ])
        preprocessed_text = preprocess_text(text)
        embedding = model.encode([preprocessed_text])
        embeddings.append(embedding[0])
        metadata.append(data)
    except Exception as e:
        print(f"Error processing data: {data.get('scheme_name', 'Unknown')}. Error: {e}")

# Save embeddings and metadata to files in Colab
np.save('/content/drive/MyDrive/Final_Year_Project/Docs/embeddings.npy', embeddings)
with open('/content/drive/MyDrive/Final_Year_Project/Docs/metadata.json', 'w', encoding='utf-8') as file:
    json.dump(metadata, file, ensure_ascii=False, indent=4)

# Download the embeddings file
from google.colab import files
files.download('/content/drive/MyDrive/Final_Year_Project/Docs/embeddings.npy')

# Download the metadata file
files.download('/content/drive/MyDrive/Final_Year_Project/Docs/metadata.json')

